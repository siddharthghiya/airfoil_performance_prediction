{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "import csv\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import imageio\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import ipdb\n",
    "import sys\n",
    "import base64\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net1, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2,20,kernel_size=(5,5),stride=(1,1))\n",
    "        nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        self.bn1 = nn.BatchNorm2d(20)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(20,10,kernel_size=(5,5),stride=(1,1))\n",
    "        nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        self.bn2 = nn.BatchNorm2d(10)\n",
    "        \n",
    "        self.fc1 = nn.Linear(1690,500)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        self.bn3 = nn.BatchNorm1d(500)\n",
    "        \n",
    "        self.fc2 = nn.Linear(500,100)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        self.bn4 = nn.BatchNorm1d(100)\n",
    "        \n",
    "        self.fc3 = nn.Linear(100,1)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(x,kernel_size=2,stride=2,padding=0,dilation=1,ceil_mode=False)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(x,kernel_size=2, stride=2, padding=0,dilation=1,ceil_mode=False)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        \n",
    "        x = x.view(x.size(0),1690)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        x = F.dropout(x,p=0.5)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "       \n",
    "        x = F.dropout(x,p=0.5)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdataloaders(data,batch_size=50):\n",
    "    np.random.shuffle(data)\n",
    "    batches=[]\n",
    "    \n",
    "    re = data[:,1].astype(np.float64)\n",
    "    re -= re.mean(0)\n",
    "    re /= re.var(0)\n",
    "    re = re.reshape(re.shape[0],1)\n",
    "    re = re.astype(np.str)\n",
    "    \n",
    "    data = np.delete(data,1,1)\n",
    "    data = np.insert(data,[1],re,axis =1)\n",
    "    \n",
    "    train_data1=data[:15000,:]\n",
    "    test_data1=data[15000:,:]\n",
    "\n",
    "    #creating train batches\n",
    "    train_batch_number = int(train_data1.shape[0]/batch_size)\n",
    "    train_data = []\n",
    "    \n",
    "    for idx in range(0,train_batch_number):\n",
    "        data = train_data1[idx*batch_size:(idx + 1)*batch_size]    \n",
    "        train_data.append(data)\n",
    "    \n",
    "    train_data = np.array(train_data)\n",
    "    \n",
    "    #creating test batches\n",
    "    test_batch_number = int(test_data1.shape[0]/batch_size)\n",
    "    test_data = []\n",
    "    \n",
    "    for idx in range(0,test_batch_number):\n",
    "        data = test_data1[idx*batch_size:(idx + 1)*batch_size,:]\n",
    "        test_data.append(data)\n",
    "        \n",
    "    test_data = np.array(test_data)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainloader, optimizer, criterion, epoch, net, scope = 'cl'):\n",
    "    \n",
    "    net.train()\n",
    "    train_loss_sum = 0\n",
    "    for idx,batch in enumerate(trainloader):\n",
    "        image_batch=[]\n",
    "\n",
    "        if scope == 'cl':\n",
    "            \n",
    "            target = (trainloader[idx,:,3]).reshape((len(trainloader[idx,:,3]),1))\n",
    "            target= target.astype(float)\n",
    "            target = torch.from_numpy(target).float()\n",
    "            \n",
    "        if scope == 'cd':\n",
    "            target = trainloader[idx,:,4].reshape((len(trainloader[idx,:,3]),1))\n",
    "            target= target.astype(float)\n",
    "            target = torch.from_numpy(target).float()\n",
    "    \n",
    "        for ndx,row in enumerate(batch):\n",
    "            image_name= trainloader[idx,ndx,0]+'_'+ str(int(float(trainloader[idx,ndx,2])))+'.png'\n",
    "            image= imageio.imread('./data_images/'+image_name)\n",
    "            re_array=np.full((64,64),trainloader[idx,ndx,1])\n",
    "            image=np.dstack((re_array,image))\n",
    "            image_batch.append(image)\n",
    "        image_batch=np.array(image_batch,dtype = float)\n",
    "        image_batch=np.transpose(image_batch,(0,3,1,2))   \n",
    "        \n",
    "        #feeding input to the network\n",
    "        image_batch = torch.from_numpy(image_batch).float()\n",
    "        output = net(image_batch)\n",
    "        loss = criterion(output,target)\n",
    "        #print(\"Train loss\", loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        train_loss_sum += float(loss)\n",
    "        if idx % 50 == 0:\n",
    "            print('epoch ', epoch, ' batch_no ', idx, ' Train loss ', loss)\n",
    "\n",
    "    train_loss = train_loss_sum/trainloader.shape[0]\n",
    "    print(\"epoch: \", epoch,\" epoch_train_loss: \", train_loss)       \n",
    "    return train_loss\n",
    "\n",
    "def test(testloader, criterion, epoch, epochs, net, scope = 'cl'):\n",
    "    net.eval()\n",
    "    test_loss_sum = 0    \n",
    "    last_epoch=[]\n",
    "    \n",
    "    for idx,batch in enumerate(testloader):\n",
    "        image_batch=[]\n",
    "        if epoch == epochs-1:\n",
    "            last_batch = batch\n",
    "        \n",
    "        if scope == 'cl':\n",
    "            target = testloader[idx,:,3].reshape((len(testloader[idx,:,3]),1))\n",
    "            target= target.astype(float)\n",
    "            target = torch.from_numpy(target).float()\n",
    "        if scope == 'cd':\n",
    "            target = testloader[idx,:,4].reshape((len(testloader[idx,:,3]),1))\n",
    "            target= target.astype(float)\n",
    "            target = torch.from_numpy(target).float()\n",
    "    \n",
    "        for ndx,row in enumerate(batch):\n",
    "            image_name = testloader[idx,ndx,0]+'_'+ str(int(float(testloader[idx,ndx,2])))+'.png'\n",
    "            image = imageio.imread('./data_images/'+image_name)\n",
    "            re_array = np.full((64,64),testloader[idx,ndx,1])\n",
    "            image = np.dstack((re_array,image))\n",
    "            image_batch.append(image)\n",
    "            \n",
    "        image_batch = np.array(image_batch,dtype = float)\n",
    "        image_batch = np.transpose(image_batch,(0,3,1,2))\n",
    "        image_batch = torch.from_numpy(image_batch).float()\n",
    "        \n",
    "        #feeding input to the network\n",
    "        output = net(image_batch)\n",
    "        \n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "        test_loss_sum += float(loss)\n",
    "        \n",
    "        if epoch == epochs-1:\n",
    "            output = output.detach().numpy()\n",
    "            last_batch = np.insert(last_batch,[5],output,axis =1) # adding prediction column for the batch \n",
    "            last_epoch.append(last_batch) # appending all the batches in the last epoch list\n",
    "\n",
    "        else:\n",
    "            last_epoch = 5\n",
    "\n",
    "        if idx % 20 == 0:\n",
    "            print('epoch ', epoch, ' batch_no ', idx, ' Test loss ', loss)\n",
    "\n",
    "    test_loss = test_loss_sum/testloader.shape[0]  \n",
    "    print(\"epoch: \", epoch, ' epoch_test_loss: ', test_loss)       \n",
    "    return test_loss, last_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  batch_no  0  Train loss  tensor(1.5396, grad_fn=<MseLossBackward>)\n",
      "epoch  0  batch_no  50  Train loss  tensor(0.0609, grad_fn=<MseLossBackward>)\n",
      "epoch  0  batch_no  100  Train loss  tensor(0.0286, grad_fn=<MseLossBackward>)\n",
      "epoch  0  batch_no  150  Train loss  tensor(0.0447, grad_fn=<MseLossBackward>)\n",
      "epoch  0  batch_no  200  Train loss  tensor(0.0422, grad_fn=<MseLossBackward>)\n",
      "epoch  0  batch_no  250  Train loss  tensor(0.0279, grad_fn=<MseLossBackward>)\n",
      "epoch:  0  epoch_train_loss:  0.059057296244427564\n",
      "epoch  0  batch_no  0  Test loss  tensor(0.0285, grad_fn=<MseLossBackward>)\n",
      "epoch  0  batch_no  20  Test loss  tensor(0.0234, grad_fn=<MseLossBackward>)\n",
      "epoch  0  batch_no  40  Test loss  tensor(0.0297, grad_fn=<MseLossBackward>)\n",
      "epoch  0  batch_no  60  Test loss  tensor(0.0240, grad_fn=<MseLossBackward>)\n",
      "epoch  0  batch_no  80  Test loss  tensor(0.0307, grad_fn=<MseLossBackward>)\n",
      "epoch:  0  epoch_test_loss:  0.02890905070429047\n",
      "epoch  1  batch_no  0  Train loss  tensor(0.0304, grad_fn=<MseLossBackward>)\n",
      "epoch  1  batch_no  50  Train loss  tensor(0.0220, grad_fn=<MseLossBackward>)\n",
      "epoch  1  batch_no  100  Train loss  tensor(0.0168, grad_fn=<MseLossBackward>)\n",
      "epoch  1  batch_no  150  Train loss  tensor(0.0157, grad_fn=<MseLossBackward>)\n",
      "epoch  1  batch_no  200  Train loss  tensor(0.0369, grad_fn=<MseLossBackward>)\n",
      "epoch  1  batch_no  250  Train loss  tensor(0.0256, grad_fn=<MseLossBackward>)\n",
      "epoch:  1  epoch_train_loss:  0.027084927797938387\n",
      "epoch  1  batch_no  0  Test loss  tensor(0.0411, grad_fn=<MseLossBackward>)\n",
      "epoch  1  batch_no  20  Test loss  tensor(0.0372, grad_fn=<MseLossBackward>)\n",
      "epoch  1  batch_no  40  Test loss  tensor(0.0516, grad_fn=<MseLossBackward>)\n",
      "epoch  1  batch_no  60  Test loss  tensor(0.0447, grad_fn=<MseLossBackward>)\n",
      "epoch  1  batch_no  80  Test loss  tensor(0.0375, grad_fn=<MseLossBackward>)\n",
      "epoch:  1  epoch_test_loss:  0.046444930508732794\n",
      "epoch  2  batch_no  0  Train loss  tensor(0.0512, grad_fn=<MseLossBackward>)\n",
      "epoch  2  batch_no  50  Train loss  tensor(0.0269, grad_fn=<MseLossBackward>)\n",
      "epoch  2  batch_no  100  Train loss  tensor(0.0278, grad_fn=<MseLossBackward>)\n",
      "epoch  2  batch_no  150  Train loss  tensor(0.0237, grad_fn=<MseLossBackward>)\n",
      "epoch  2  batch_no  200  Train loss  tensor(0.0240, grad_fn=<MseLossBackward>)\n",
      "epoch  2  batch_no  250  Train loss  tensor(0.0272, grad_fn=<MseLossBackward>)\n",
      "epoch:  2  epoch_train_loss:  0.024272410878911615\n",
      "epoch  2  batch_no  0  Test loss  tensor(0.0184, grad_fn=<MseLossBackward>)\n",
      "epoch  2  batch_no  20  Test loss  tensor(0.0194, grad_fn=<MseLossBackward>)\n",
      "epoch  2  batch_no  40  Test loss  tensor(0.0262, grad_fn=<MseLossBackward>)\n",
      "epoch  2  batch_no  60  Test loss  tensor(0.0172, grad_fn=<MseLossBackward>)\n",
      "epoch  2  batch_no  80  Test loss  tensor(0.0195, grad_fn=<MseLossBackward>)\n",
      "epoch:  2  epoch_test_loss:  0.019702218234952953\n",
      "epoch  3  batch_no  0  Train loss  tensor(0.0203, grad_fn=<MseLossBackward>)\n",
      "epoch  3  batch_no  50  Train loss  tensor(0.0232, grad_fn=<MseLossBackward>)\n",
      "epoch  3  batch_no  100  Train loss  tensor(0.0106, grad_fn=<MseLossBackward>)\n",
      "epoch  3  batch_no  150  Train loss  tensor(0.0155, grad_fn=<MseLossBackward>)\n",
      "epoch  3  batch_no  200  Train loss  tensor(0.0227, grad_fn=<MseLossBackward>)\n",
      "epoch  3  batch_no  250  Train loss  tensor(0.0192, grad_fn=<MseLossBackward>)\n",
      "epoch:  3  epoch_train_loss:  0.01579783740763863\n",
      "epoch  3  batch_no  0  Test loss  tensor(0.0193, grad_fn=<MseLossBackward>)\n",
      "epoch  3  batch_no  20  Test loss  tensor(0.0184, grad_fn=<MseLossBackward>)\n",
      "epoch  3  batch_no  40  Test loss  tensor(0.0158, grad_fn=<MseLossBackward>)\n",
      "epoch  3  batch_no  60  Test loss  tensor(0.0202, grad_fn=<MseLossBackward>)\n",
      "epoch  3  batch_no  80  Test loss  tensor(0.0187, grad_fn=<MseLossBackward>)\n",
      "epoch:  3  epoch_test_loss:  0.014585365779283974\n",
      "epoch  4  batch_no  0  Train loss  tensor(0.0130, grad_fn=<MseLossBackward>)\n",
      "epoch  4  batch_no  50  Train loss  tensor(0.0136, grad_fn=<MseLossBackward>)\n",
      "epoch  4  batch_no  100  Train loss  tensor(0.0113, grad_fn=<MseLossBackward>)\n",
      "epoch  4  batch_no  150  Train loss  tensor(0.0119, grad_fn=<MseLossBackward>)\n",
      "epoch  4  batch_no  200  Train loss  tensor(0.0115, grad_fn=<MseLossBackward>)\n",
      "epoch  4  batch_no  250  Train loss  tensor(0.0087, grad_fn=<MseLossBackward>)\n",
      "epoch:  4  epoch_train_loss:  0.012670513149350882\n",
      "epoch  4  batch_no  0  Test loss  tensor(0.0122, grad_fn=<MseLossBackward>)\n",
      "epoch  4  batch_no  20  Test loss  tensor(0.0140, grad_fn=<MseLossBackward>)\n",
      "epoch  4  batch_no  40  Test loss  tensor(0.0149, grad_fn=<MseLossBackward>)\n",
      "epoch  4  batch_no  60  Test loss  tensor(0.0117, grad_fn=<MseLossBackward>)\n",
      "epoch  4  batch_no  80  Test loss  tensor(0.0131, grad_fn=<MseLossBackward>)\n",
      "epoch:  4  epoch_test_loss:  0.012422761124455266\n"
     ]
    }
   ],
   "source": [
    "batch_number = 50\n",
    "learning_rate = 0.0001\n",
    "epochs = 5\n",
    "\n",
    "data=np.load(\"data.npy\")\n",
    "train_data, test_data = getdataloaders(data)\n",
    "\n",
    "\n",
    "#creating the network and defining loss function\n",
    "cnn_foil = Net1()\n",
    "optimizer = torch.optim.Adam(cnn_foil.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "epoch_list = []\n",
    "\n",
    "#training the network\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_data, optimizer, criterion, epoch, cnn_foil, scope = 'cl')\n",
    "    test_loss, output_data = test(test_data, criterion, epoch,epochs, cnn_foil, scope = 'cl')\n",
    "\n",
    "    train_loss_list.append(train_loss)\n",
    "    test_loss_list.append(test_loss)\n",
    "\n",
    "    epoch_list.append(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data=np.array(output_data)\n",
    "np.save(\"Foil_CNN_prediction\",output_data)\n",
    "output_data=output_data.reshape((4500,6))\n",
    "output_data=output_data.astype(str)\n",
    "np.savetxt(\"Foil_CNN_prediction.csv\", output_data, delimiter=\",\", fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train loss tanh\",train_loss_list)\n",
    "np.save(\"test loss tanh\",test_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
